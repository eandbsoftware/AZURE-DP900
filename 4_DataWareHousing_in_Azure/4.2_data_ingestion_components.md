
# data ingestion components:

## Azure Data Factory

- Azure Data Factory is a data ingestion, transformation and orchestration-managed service on the cloud for data engineers, perfect for data integration workflows.
- It can ingest large amount of raw data from several sources, both on premises and in the cloud.
- It has connectors for most Azure services and even services from cloud competitors, such as Google and AWS, dozens of relational and non-relational database and data warehouses, such as SQL Server, Oracle, Cassandra, MongoDB and SAP HANA, several SaaS applications, such as Dynamics, Jira and Salesforce
- supports several file formats, including JSON, XML, Parquet, Avro and ORC.
- can also clean, transform and restructure data, as well as filter out data that might be corrupt or duplicated, supporting therefore both ETL and ELT processes.
- These transformation tasks can be done by Azure Data Factory itself through a feature called **mapping data flows**. However, these transformations can also be done by other Azure services, such as **Databricks, and HDInsight.**

### main components of Data Factory

- **pipeline**
  - logical group of activities that perform unit of work
  - its the actual work that we do in data factory 
  - can be created both via GUI or through code
  - **Activities in pipeline:**
    - **Data movement activities** move data between a source and a destination. It's also called **sink**.For example, a copy from SQL Server to Cosmos DB.
    - **Data transformation activities** perform some change on the data.
      - This data change could be executed by the Data Factory itself, which is called mapping data flow or by calling an external compute resource, such as Databricks or Hive
    - **control flow activities**  which are a way to implement coding logic on your pipeline, such as pinging variable or executing a loop etc.

- **Integration runtimes** 
  - these are the compute infrastructure of Data Factory, which is needed to execute your activities.
- **linked services**
  - A linked service provides the information that you need to connect to the source, a destination or a compute resource.
  - they basically tell you where to find your external data or service.

- **datasets**
  - representation of the data that you're working with.
  - While linked services tell you where to find the data, datasets tells you its details and structure and format, such as JSON or XML.

- **triggers**: 
  - Data Factory component that initiate the execution of a pipeline
  - example: event based triggers

## SSIS or SQL Server Integration Services.

- SSIS is the on-premises counterpart of Data Factory and it's part of SQL Server.
- can add Azure support to SSIS by installing the Azure feature pack for SSIS.
- you can also run your existing SSIS packages on Azure Data Factory, which is useful for migration scenarios
- scalability is limited to the performance of the server where SSIS is installed.


## PolyBase

- feature of both SQL Server and Azure Synapse Analytics that enables you to run Transact-SQL commands on external data sources, such as Azure Data Lake, Blob Storage, Hadoop or Spark just as if they were SQL tables.
- 
## data lakes

- repository for large amounts of raw data.
- semi-structured or unstructured.
- used as a staging layer for your ingested data before this data is structured and loaded into a final destination, which is generally a data warehouse solution.
- **2 main services:**
  - **Azure Data Lake Storage** 
    - Azure Data Lake Storage provides a file repository that can store near unlimited amounts of data.
    - compatible with the Hadoop Distributed File System, HDFS, and can be accessed directly by Azure Data Factory, Databricks, HDInsight, Data Lake Analytics and Stream Analytics
    - Ideally, you should place your data lake on the same Azure data center as your analytics tools, otherwise you incur bandwidth costs as the data transverses the regions
    - 
  - **Azure Data Lake Analytics**
    - Azure Data Lake Analytics is an on-demand analytics job service that you can use to process big data.
    - has a set of tools that allow you to create jobs that can transform data and extracting sites.
    - You write those jobs using U-SQL, which is a hybrid programming language that mixes SQL and C#.